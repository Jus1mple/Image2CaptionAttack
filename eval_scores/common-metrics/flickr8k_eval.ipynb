{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from pycocoevalcap.eval import COCOEvalCap\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io as io\n",
    "import pylab\n",
    "\n",
    "pylab.rcParams[\"figure.figsize\"] = (16, 7.5)\n",
    "\n",
    "import json\n",
    "from json import encoder\n",
    "import numpy as np\n",
    "\n",
    "encoder.FLOAT_REPR = lambda o: format(o, \".3f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "blip2_model = \"blip2-opt-2.7b\"\n",
    "clip_models = [\"ViT-32B\", \"ViT-16B\", \"RN50\", \"RN101\"]\n",
    "leaked_feature_layers = {\n",
    "    \"ViT-16B\": [\n",
    "        \"vit-base\", \n",
    "        # \"vit-no-proj\"\n",
    "    ],\n",
    "    \"ViT-32B\": [\n",
    "        \"vit-base\", \n",
    "        # \"vit-no-proj\"\n",
    "    ],\n",
    "    \"RN50\": [\n",
    "        \"resnet-base\",\n",
    "        # \"resnet-layer1\",\n",
    "        # \"resnet-layer2\",\n",
    "        # \"resnet-layer3\",\n",
    "        # \"resnet-layer4\",\n",
    "    ],\n",
    "    \"RN101\": [\n",
    "        \"resnet-base\",\n",
    "        # \"resnet-layer1\",\n",
    "        # \"resnet-layer2\",\n",
    "        # \"resnet-layer3\",\n",
    "        # \"resnet-layer4\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.04s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "# run for all\n",
    "# annFile = r\"H:\\Code\\AutoDL_backup\\datasets\\image_caption_generation\\flickr8k\\annotations\\captions_val.json\"\n",
    "annFile = \"/root/autodl-tmp/datasets/image_caption_generation/flickr8k/annotations/captions_val.json\"\n",
    "coco = COCO(annFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 102870 tokens at 558347.59 tokens per second.\n",
      "PTBTokenizer tokenized 11341 tokens at 148459.41 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 15\u001b[0m\n\u001b[1;32m      8\u001b[0m cocoEval \u001b[38;5;241m=\u001b[39m COCOEvalCap(coco, cocoRes)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# evaluate on a subset of images by setting\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# cocoEval.params['image_id'] = cocoRes.getImgIds()\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# please remove this line when evaluating the full validation set\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# cocoEval.params[\"image_id\"] = cocoRes.getImgIds()\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mcocoEval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/blip2/lib/python3.10/site-packages/pycocoevalcap/eval.py:53\u001b[0m, in \u001b[0;36mCOCOEvalCap.evaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m scorer, method \u001b[38;5;129;01min\u001b[39;00m scorers:\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomputing \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m score...\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m%\u001b[39m(scorer\u001b[38;5;241m.\u001b[39mmethod()))\n\u001b[0;32m---> 53\u001b[0m     score, scores \u001b[38;5;241m=\u001b[39m \u001b[43mscorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(method) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m sc, scs, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(score, scores, method):\n",
      "File \u001b[0;32m~/miniconda3/envs/blip2/lib/python3.10/site-packages/pycocoevalcap/bleu/bleu.py:23\u001b[0m, in \u001b[0;36mBleu.compute_score\u001b[0;34m(self, gts, res, verbose)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_score\u001b[39m(\u001b[38;5;28mself\u001b[39m, gts, res, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m(gts\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;241m==\u001b[39m res\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m     24\u001b[0m     imgIds \u001b[38;5;241m=\u001b[39m gts\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m     26\u001b[0m     bleu_scorer \u001b[38;5;241m=\u001b[39m BleuScorer(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset_name = \"flickr8k\"\n",
    "clip_model = \"RN50\"\n",
    "leaked_feature_layer = \"resnet-layer1\"\n",
    "# add_noise = True\n",
    "resFile = f\"../../processed_results/results_{dataset_name}_{blip2_model}_{clip_model}_{leaked_feature_layer}_noise.json\"\n",
    "cocoRes = coco.loadRes(resFile)\n",
    "# create cocoEval object by taking coco and cocoRes\n",
    "cocoEval = COCOEvalCap(coco, cocoRes)\n",
    "\n",
    "# evaluate on a subset of images by setting\n",
    "# cocoEval.params['image_id'] = cocoRes.getImgIds()\n",
    "# please remove this line when evaluating the full validation set\n",
    "# cocoEval.params[\"image_id\"] = cocoRes.getImgIds()\n",
    "\n",
    "cocoEval.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"flickr8k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clip_model in clip_models:\n",
    "    for leaked_feature_layer in leaked_feature_layers[clip_model]:\n",
    "        print(f\"Model: {blip2_model}, CLIP: {clip_model}, Layer: {leaked_feature_layer}\")\n",
    "        resFile = f\"../../processed_results/results_{dataset_name}_{blip2_model}_{clip_model}_{leaked_feature_layer}.json\"\n",
    "        cocoRes = coco.loadRes(resFile)\n",
    "        # create cocoEval object by taking coco and cocoRes\n",
    "        cocoEval = COCOEvalCap(coco, cocoRes)\n",
    "\n",
    "        # evaluate on a subset of images by setting\n",
    "        # cocoEval.params['image_id'] = cocoRes.getImgIds()\n",
    "        # please remove this line when evaluating the full validation set\n",
    "        cocoEval.params[\"image_id\"] = cocoRes.getImgIds()\n",
    "\n",
    "        # evaluate results\n",
    "        # SPICE will take a few minutes the first time, but speeds up due to caching\n",
    "        cocoEval.evaluate()\n",
    "\n",
    "        # print output evaluation scores\n",
    "        for metric, score in cocoEval.eval.items():\n",
    "            print(\"%s: %.3f\" % (metric, score))\n",
    "\n",
    "        # for metric in [\n",
    "        #     \"Bleu_1\",\n",
    "        #     \"Bleu_2\",\n",
    "        #     \"Bleu_3\",\n",
    "        #     \"Bleu_4\",\n",
    "        #     \"METEOR\",\n",
    "        #     \"ROUGE_L\",\n",
    "        #     \"CIDEr\",\n",
    "        #     \"SPICE\",\n",
    "        # ]:\n",
    "        #     scores = [eva[metric] for eva in cocoEval.evalImgs]\n",
    "        #     print(f\"{metric}: {scores}\")\n",
    "        #     plt.figure(figsize=(10, 8))\n",
    "        #     plt.hist(scores)\n",
    "        #     plt.title(f\"Histogram of {metric} Scores\", fontsize=20)\n",
    "        #     plt.xlabel(f\"{metric} score\", fontsize=20)\n",
    "        #     plt.ylabel(\"result counts\", fontsize=20)\n",
    "        #     # plt.show()\n",
    "        #     plt.savefig(\n",
    "        #         f\"/root/Image2CaptionAttack/plot/figs/common_metrics/{metric}_{dataset_name}_{blip2_model}_{clip_model}_{leaked_feature_layer}.pdf\",\n",
    "        #         dpi=600,\n",
    "        #         bbox_inches=\"tight\",\n",
    "        #     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 12674 tokens at 169870.95 tokens per second.\n",
      "PTBTokenizer tokenized 12276 tokens at 168186.98 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 10277, 'reflen': 10506, 'guess': [10277, 9277, 8277, 7277], 'correct': [2249, 316, 63, 20]}\n",
      "ratio: 0.978202931658007\n",
      "Bleu_1: 0.214\n",
      "Bleu_2: 0.084\n",
      "Bleu_3: 0.038\n",
      "Bleu_4: 0.019\n",
      "computing METEOR score...\n",
      "METEOR: 0.081\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.186\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.194\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 1.456 s\n",
      "SPICE: 0.087\n",
      "Bleu_1: 0.214\n",
      "Bleu_2: 0.084\n",
      "Bleu_3: 0.038\n",
      "Bleu_4: 0.019\n",
      "METEOR: 0.081\n",
      "ROUGE_L: 0.186\n",
      "CIDEr: 0.194\n",
      "SPICE: 0.087\n"
     ]
    }
   ],
   "source": [
    "resFile = f\"../../processed_results/results_flickr8k_{blip2_model}_RN50_resnet-layer2.json\"\n",
    "cocoRes = coco.loadRes(resFile)\n",
    "# create cocoEval object by taking coco and cocoRes\n",
    "cocoEval = COCOEvalCap(coco, cocoRes)\n",
    "\n",
    "# evaluate on a subset of images by setting\n",
    "\n",
    "# cocoEval.params['image_id'] = cocoRes.getImgIds()\n",
    "# please remove this line when evaluating the full validation set\n",
    "cocoEval.params[\"image_id\"] = cocoRes.getImgIds()\n",
    "\n",
    "# evaluate results\n",
    "# SPICE will take a few minutes the first time, but speeds up due to caching\n",
    "cocoEval.evaluate()\n",
    "\n",
    "for metric, score in cocoEval.eval.items():\n",
    "    print(\"%s: %.3f\" % (metric, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制所有Victim model指定的metric下的分布图\n",
    "victim_models = {\n",
    "    \"ViT-16B\": \"ViT-16B\",\n",
    "    \"ViT-32B\": \"ViT-32B\",\n",
    "    \"RN50\": \"ResNet50\",\n",
    "    \"RN101\": \"ResNet101\",\n",
    "    \"mobilenetv2\": \"MobileNetV2\",\n",
    "    \"mobilenetv3-small\": \"MobileNetV3-Small\",\n",
    "    \"mobilenetv3-large\": \"MobileNetV3-Large\",\n",
    "}\n",
    "\n",
    "leaked_feature_layers = {\n",
    "    \"ViT-16B\": [\"vit-base\", \"vit-no-proj\"],\n",
    "    \"ViT-32B\": [\"vit-base\", \"vit-no-proj\"],\n",
    "    \"RN50\": [\n",
    "        \"resnet-base\",\n",
    "        \"resnet-layer1\",\n",
    "        \"resnet-layer2\",\n",
    "        \"resnet-layer3\",\n",
    "        \"resnet-layer4\",\n",
    "    ],\n",
    "    \"RN101\": [\n",
    "        \"resnet-base\",\n",
    "        \"resnet-layer1\",\n",
    "        \"resnet-layer2\",\n",
    "        \"resnet-layer3\",\n",
    "        \"resnet-layer4\",\n",
    "    ],\n",
    "    \"mobilenetv2\": [\n",
    "        \"mobilenet-base\",\n",
    "        \"mobilenet-layer1\",\n",
    "        \"mobilenet-layer-mid\",\n",
    "        \"mobilenet-all-blocks\",\n",
    "    ],\n",
    "    \"mobilenetv3-small\": [\n",
    "        \"mobilenet-base\",\n",
    "        \"mobilenet-layer1\",\n",
    "        \"mobilenet-layer-mid\",\n",
    "        \"mobilenet-all-blocks\",\n",
    "    ],\n",
    "    \"mobilenetv3-large\": [\n",
    "        \"mobilenet-base\",\n",
    "        \"mobilenet-layer1\",\n",
    "        \"mobilenet-layer-mid\",\n",
    "        \"mobilenet-all-blocks\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    \"Bleu_1\",\n",
    "    \"Bleu_2\",\n",
    "    \"Bleu_3\",\n",
    "    \"Bleu_4\",\n",
    "    \"METEOR\",\n",
    "    \"ROUGE_L\",\n",
    "    \"CIDEr\",\n",
    "    \"SPICE\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "blip2_model = \"blip2-opt-2.7b\"\n",
    "dst_name = \"flickr8k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annFile = \"/root/autodl-tmp/datasets/image_caption_generation/flickr8k/annotations/captions_val.json\"\n",
    "coco = COCO(annFile)\n",
    "\n",
    "eval_val_dict = {}\n",
    "\n",
    "for victim_model in victim_models:\n",
    "    vic_val_dict = {}\n",
    "    leaked_feature_layer = leaked_feature_layers[victim_model][0]\n",
    "    resFile = f\"/root/Image2CaptionAttack/processed_results/results_{dst_name}_{blip2_model}_{victim_model}_{leaked_feature_layer}.json\"\n",
    "    cocoRes = coco.loadRes(resFile)\n",
    "    # create cocoEval object by taking coco and cocoRes\n",
    "    cocoEval = COCOEvalCap(coco, cocoRes)\n",
    "\n",
    "    # evaluate on a subset of images by setting\n",
    "    # cocoEval.params['image_id'] = cocoRes.getImgIds()\n",
    "    # please remove this line when evaluating the full validation set\n",
    "    cocoEval.params[\"image_id\"] = cocoRes.getImgIds()\n",
    "\n",
    "    # evaluate results\n",
    "    # SPICE will take a few minutes the first time, but speeds up due to caching\n",
    "    cocoEval.evaluate()\n",
    "    for metric in metrics:\n",
    "        scores = [eva[metric] for eva in cocoEval.evalImgs]\n",
    "        vic_val_dict[metric] = scores\n",
    "    eval_val_dict[victim_model] = vic_val_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.04s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 12674 tokens at 190772.58 tokens per second.\n",
      "PTBTokenizer tokenized 12112 tokens at 204252.90 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 10106, 'reflen': 10506, 'guess': [10106, 9106, 8106, 7106], 'correct': [3116, 746, 216, 73]}\n",
      "ratio: 0.961926518179996\n",
      "Bleu_1: 0.296\n",
      "Bleu_2: 0.153\n",
      "Bleu_3: 0.084\n",
      "Bleu_4: 0.049\n",
      "computing METEOR score...\n",
      "METEOR: 0.127\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.271\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.535\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 1.223 s\n",
      "SPICE: 0.190\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 12674 tokens at 195430.09 tokens per second.\n",
      "PTBTokenizer tokenized 12660 tokens at 210693.55 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 10661, 'reflen': 10506, 'guess': [10661, 9661, 8661, 7661], 'correct': [2870, 613, 159, 44]}\n",
      "ratio: 1.0147534742051194\n",
      "Bleu_1: 0.269\n",
      "Bleu_2: 0.131\n",
      "Bleu_3: 0.068\n",
      "Bleu_4: 0.037\n",
      "computing METEOR score...\n",
      "METEOR: 0.112\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.241\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.375\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 1.139 s\n",
      "SPICE: 0.159\n"
     ]
    }
   ],
   "source": [
    "victim_model = \"ViT-16B\"\n",
    "\n",
    "annFile = \"/root/autodl-tmp/datasets/image_caption_generation/flickr8k/annotations/captions_val.json\"\n",
    "coco = COCO(annFile)\n",
    "\n",
    "\n",
    "eval_val_dict_leaked_feature_layer = {}\n",
    "\n",
    "for leaked_feature_layer in leaked_feature_layers[victim_model]:\n",
    "    resFile = f\"/root/Image2CaptionAttack/processed_results/results_{dst_name}_{blip2_model}_{victim_model}_{leaked_feature_layer}.json\"\n",
    "    cocoRes = coco.loadRes(resFile)\n",
    "    # create cocoEval object by taking coco and cocoRes\n",
    "    cocoEval = COCOEvalCap(coco, cocoRes)\n",
    "\n",
    "    val_dict = {}\n",
    "\n",
    "    # evaluate on a subset of images by setting\n",
    "    # cocoEval.params['image_id'] = cocoRes.getImgIds()\n",
    "    # please remove this line when evaluating the full validation set\n",
    "    cocoEval.params[\"image_id\"] = cocoRes.getImgIds()\n",
    "\n",
    "    # evaluate results\n",
    "    # SPICE will take a few minutes the first time, but speeds up due to caching\n",
    "    cocoEval.evaluate()\n",
    "    for metric in metrics:\n",
    "        scores = [eva[metric] for eva in cocoEval.evalImgs]\n",
    "        val_dict[metric] = scores\n",
    "\n",
    "    eval_val_dict_leaked_feature_layer[leaked_feature_layer] = val_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blip2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
