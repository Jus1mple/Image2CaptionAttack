{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from pycocoevalcap.eval import COCOEvalCap\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io as io\n",
    "import pylab\n",
    "\n",
    "pylab.rcParams[\"figure.figsize\"] = (16, 7.5)\n",
    "\n",
    "import json\n",
    "from json import encoder\n",
    "import numpy as np\n",
    "\n",
    "encoder.FLOAT_REPR = lambda o: format(o, \".3f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "blip2_model = \"blip2-opt-2.7b\"\n",
    "clip_model = \"ViT-32B\"\n",
    "leaked_feature_layer = \"vit-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 绘制所有Victim model指定的metric下的分布图\n",
    "victim_models = {\n",
    "    \"ViT-16B\": \"ViT-16B\",\n",
    "    \"ViT-32B\": \"ViT-32B\",\n",
    "    \"RN50\": \"ResNet50\",\n",
    "    \"RN101\": \"ResNet101\",\n",
    "    \"mobilenetv2\": \"MobileNetV2\",\n",
    "    \"mobilenetv3-small\": \"MobileNetV3-Small\",\n",
    "    \"mobilenetv3-large\": \"MobileNetV3-Large\",\n",
    "}\n",
    "\n",
    "leaked_feature_layers = {\n",
    "    \"ViT-16B\": [\"vit-base\", \"vit-no-proj\"],\n",
    "    \"ViT-32B\": [\"vit-base\", \"vit-no-proj\"],\n",
    "    \"RN50\": [\n",
    "        \"resnet-base\",\n",
    "        \"resnet-layer1\",\n",
    "        \"resnet-layer2\",\n",
    "        \"resnet-layer3\",\n",
    "        \"resnet-layer4\",\n",
    "    ],\n",
    "    \"RN101\": [\n",
    "        \"resnet-base\",\n",
    "        \"resnet-layer1\",\n",
    "        \"resnet-layer2\",\n",
    "        \"resnet-layer3\",\n",
    "        \"resnet-layer4\",\n",
    "    ],\n",
    "    \"mobilenetv2\": [\n",
    "        \"mobilenet-base\",\n",
    "        \"mobilenet-layer1\",\n",
    "        \"mobilenet-layer-mid\",\n",
    "        \"mobilenet-all-blocks\",\n",
    "    ],\n",
    "    \"mobilenetv3-small\": [\n",
    "        \"mobilenet-base\",\n",
    "        \"mobilenet-layer1\",\n",
    "        \"mobilenet-layer-mid\",\n",
    "        \"mobilenet-all-blocks\",\n",
    "    ],\n",
    "    \"mobilenetv3-large\": [\n",
    "        \"mobilenet-base\",\n",
    "        \"mobilenet-layer1\",\n",
    "        \"mobilenet-layer-mid\",\n",
    "        \"mobilenet-all-blocks\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    \"Bleu_1\",\n",
    "    \"Bleu_2\",\n",
    "    \"Bleu_3\",\n",
    "    \"Bleu_4\",\n",
    "    \"METEOR\",\n",
    "    \"ROUGE_L\",\n",
    "    \"CIDEr\",\n",
    "    \"SPICE\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "blip2_model = \"blip2-opt-2.7b\"\n",
    "dst_name = \"imagenet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annFile = \"/root/autodl-tmp/datasets/image_caption_generation/benjamin-paine/imagenet-1k-256x256/test/caption_test.json\"\n",
    "coco = COCO(annFile)\n",
    "\n",
    "eval_val_dict = {}\n",
    "\n",
    "for victim_model in victim_models:\n",
    "    vic_val_dict = {}\n",
    "    leaked_feature_layer = leaked_feature_layers[victim_model][0]\n",
    "    resFile = f\"/root/Image2CaptionAttack/processed_results/results_{dst_name}_{blip2_model}_{victim_model}_{leaked_feature_layer}.json\"\n",
    "    cocoRes = coco.loadRes(resFile)\n",
    "    # create cocoEval object by taking coco and cocoRes\n",
    "    cocoEval = COCOEvalCap(coco, cocoRes)\n",
    "\n",
    "    # evaluate on a subset of images by setting\n",
    "    # cocoEval.params['image_id'] = cocoRes.getImgIds()\n",
    "    # please remove this line when evaluating the full validation set\n",
    "    cocoEval.params[\"image_id\"] = cocoRes.getImgIds()\n",
    "\n",
    "    # evaluate results\n",
    "    # SPICE will take a few minutes the first time, but speeds up due to caching\n",
    "    cocoEval.evaluate()\n",
    "    for metric in metrics:\n",
    "        scores = [eva[metric] for eva in cocoEval.evalImgs]\n",
    "        vic_val_dict[metric] = scores\n",
    "    eval_val_dict[victim_model] = vic_val_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 12630 tokens at 89823.45 tokens per second.\n",
      "PTBTokenizer tokenized 13055 tokens at 167217.46 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 10843, 'reflen': 10430, 'guess': [10843, 9943, 9043, 8144], 'correct': [4832, 1998, 877, 412]}\n",
      "ratio: 1.039597315436142\n",
      "Bleu_1: 0.446\n",
      "Bleu_2: 0.299\n",
      "Bleu_3: 0.206\n",
      "Bleu_4: 0.145\n",
      "computing METEOR score...\n",
      "METEOR: 0.183\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.412\n",
      "computing CIDEr score...\n",
      "CIDEr: 1.182\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 1.295 s\n",
      "SPICE: 0.202\n",
      "Loading and preparing results...\n",
      "DONE (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "tokenization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PTBTokenizer tokenized 12630 tokens at 179862.21 tokens per second.\n",
      "PTBTokenizer tokenized 13290 tokens at 201069.69 tokens per second.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting up scorers...\n",
      "computing Bleu score...\n",
      "{'testlen': 11098, 'reflen': 10430, 'guess': [11098, 10198, 9298, 8398], 'correct': [4514, 1648, 673, 300]}\n",
      "ratio: 1.0640460210928988\n",
      "Bleu_1: 0.407\n",
      "Bleu_2: 0.256\n",
      "Bleu_3: 0.168\n",
      "Bleu_4: 0.114\n",
      "computing METEOR score...\n",
      "METEOR: 0.161\n",
      "computing Rouge score...\n",
      "ROUGE_L: 0.378\n",
      "computing CIDEr score...\n",
      "CIDEr: 0.865\n",
      "computing SPICE score...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing reference captions\n",
      "Parsing test captions\n",
      "Initiating Stanford parsing pipeline\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator tokenize\n",
      "[main] INFO edu.stanford.nlp.pipeline.TokenizerAnnotator - TokenizerAnnotator: No tokenizer type provided. Defaulting to PTBTokenizer.\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ssplit\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator parse\n",
      "[main] INFO edu.stanford.nlp.parser.common.ParserGrammar - Loading parser from serialized file edu/stanford/nlp/models/lexparser/englishPCFG.ser.gz ... \n",
      "done [0.4 sec].\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator lemma\n",
      "[main] INFO edu.stanford.nlp.pipeline.StanfordCoreNLP - Adding annotator ner\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.all.3class.distsim.crf.ser.gz ... done [1.4 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.muc.7class.distsim.crf.ser.gz ... done [0.5 sec].\n",
      "Loading classifier from edu/stanford/nlp/models/ner/english.conll.4class.distsim.crf.ser.gz ... done [0.8 sec].\n",
      "Threads( StanfordCoreNLP ) [8.428 seconds]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPICE evaluation took: 15.02 s\n",
      "SPICE: 0.161\n"
     ]
    }
   ],
   "source": [
    "victim_model = \"ViT-16B\"\n",
    "\n",
    "annFile = \"/root/autodl-tmp/datasets/image_caption_generation/benjamin-paine/imagenet-1k-256x256/test/caption_test.json\"\n",
    "coco = COCO(annFile)\n",
    "\n",
    "eval_val_dict_leaked_feature_layer = {}\n",
    "\n",
    "for leaked_feature_layer in leaked_feature_layers[victim_model]:\n",
    "    resFile = f\"/root/Image2CaptionAttack/processed_results/results_{dst_name}_{blip2_model}_{victim_model}_{leaked_feature_layer}.json\"\n",
    "    cocoRes = coco.loadRes(resFile)\n",
    "    # create cocoEval object by taking coco and cocoRes\n",
    "    cocoEval = COCOEvalCap(coco, cocoRes)\n",
    "\n",
    "    val_dict = {}\n",
    "\n",
    "    # evaluate on a subset of images by setting\n",
    "    # cocoEval.params['image_id'] = cocoRes.getImgIds()\n",
    "    # please remove this line when evaluating the full validation set\n",
    "    cocoEval.params[\"image_id\"] = cocoRes.getImgIds()\n",
    "\n",
    "    # evaluate results\n",
    "    # SPICE will take a few minutes the first time, but speeds up due to caching\n",
    "    cocoEval.evaluate()\n",
    "    for metric in metrics:\n",
    "        scores = [eva[metric] for eva in cocoEval.evalImgs]\n",
    "        val_dict[metric] = scores\n",
    "\n",
    "    eval_val_dict_leaked_feature_layer[leaked_feature_layer] = val_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blip2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
